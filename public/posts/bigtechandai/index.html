<!DOCTYPE html>
<html lang="en" dir="auto">

<head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>The AI Panic: Big Tech, Open-source and Monopoly Power | Privacy, Security and the Law</title>
<meta name="keywords" content="">
<meta name="description" content="Big Tech, billionaires and whole lot of Artificial Intelligence lobbying.">
<meta name="author" content="Luke Arbuthnot">
<link rel="canonical" href="http://localhost:1313/posts/bigtechandai/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.0802077b66898150abd59d17118bc49c7c23c4c086d3ca01aebbe12875dc08f7.css" integrity="sha256-CAIHe2aJgVCr1Z0XEYvEnHwjxMCG08oBrrvhKHXcCPc=" rel="preload stylesheet" as="style">
<link rel="icon" href="http://localhost:1313/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="http://localhost:1313/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="http://localhost:1313/favicon-32x32.png">
<link rel="apple-touch-icon" href="http://localhost:1313/apple-touch-icon.png">
<link rel="mask-icon" href="http://localhost:1313/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="http://localhost:1313/posts/bigtechandai/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
</noscript>
<meta property="og:title" content="The AI Panic: Big Tech, Open-source and Monopoly Power" />
<meta property="og:description" content="Big Tech, billionaires and whole lot of Artificial Intelligence lobbying." />
<meta property="og:type" content="article" />
<meta property="og:url" content="http://localhost:1313/posts/bigtechandai/" />
<meta property="og:image" content="http://localhost:1313/images/eyewatchingmandesk.jpeg/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2024-06-29T00:00:00+00:00" />
<meta property="article:modified_time" content="2024-06-29T00:00:00+00:00" />

<meta name="twitter:card" content="summary_large_image" />
<meta name="twitter:image" content="http://localhost:1313/images/eyewatchingmandesk.jpeg/" />
<meta name="twitter:title" content="The AI Panic: Big Tech, Open-source and Monopoly Power"/>
<meta name="twitter:description" content="Big Tech, billionaires and whole lot of Artificial Intelligence lobbying."/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Privacy, Security and the Law",
      "item": "http://localhost:1313/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "The AI Panic: Big Tech, Open-source and Monopoly Power",
      "item": "http://localhost:1313/posts/bigtechandai/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "The AI Panic: Big Tech, Open-source and Monopoly Power",
  "name": "The AI Panic: Big Tech, Open-source and Monopoly Power",
  "description": "Big Tech, billionaires and whole lot of Artificial Intelligence lobbying.",
  "keywords": [
    
  ],
  "articleBody": "The Existential Threat of AI? There is currently a concerted effort to portray Artificial Intelligence developments as an extinction level risk to humanity, on par with a nuclear war and deadly pandemics. AI experts, namely leaders and CEO’s from Big Tech and adjacent industries, certain academics and billionaire backed philanthropy groups have all called for state intervention and regulation of their own industry. The mass media, quick to pick up on claims of the oncoming AI apocalypse, have regurgitated these claims from the experts, drumming up popular apprehension amongst the general public, elected officials and policy makers.\n“My worst fears are that we—the field, the technology, the industry—cause significant harm to the world. I think that can happen in a lot of different ways” - Sam Altman, ‘Open’AI\n“If somebody builds a too-powerful AI, under present conditions, I expect that every single member of the human species and all biological life on Earth dies shortly thereafter” - Eliezer Yudkowsky, Machine Intelligence Research Institute\nLooking at these presented grave dangers from industry insiders and leaders it appears to me that something does not add up. AI is purported to have the potential to be catastrophic for the human race yet AI development has not been paused. Quite to the contrary it has accelerated at breakneck pace as for-profit, Big Tech companies seek to cement and expand their market positions and develop ever more powerful AI models.\nTwo Visions of the Future of AI There is a conflict playing out as to how AI is to be developed.\nOn one side there is the nexus of Big Tech, the Surveillance Capitalists and increasingly elements of immense State power which, as put eloquently by Zuboff, “constitute a sweeping political-economic institutional order that exerts oligopolistic control over most digital information and communication spaces, systems, and processes.” These entities would fashion themselves as a technological high priesthood, gate-keeping the development and deployment of AI as regulated, restricted and walled off behind their proprietary, nontransparent systems. This is not new. Large technology corporations and States have historically been opposed to open source and democratized technology as it challenges their institutional and monopoly powers within society.\nOn the other side, there are a growing number of individuals, companies (including some surprises such as Meta and IBM) and parts of civil society that reject this view and see open sourced Artificial Intelligence as necessary for innovation and safety through transparency, effective competition and security. As seen with LAION’s “Call to Protect Open-Source AI in Europe,” the touted benefits of open-source AI are security, competition and safety:\n“First, open-source AI promotes safety through transparency. Open-sourcing data, models, and workflows enables researchers and authorities to audit the performance of a model or system; develop interpretability techniques; identify risks; and establish mitigations or develop anticipatory countermeasures. Second, open-source AI promotes competition. Small to medium enterprises across Europe can build on open-source models to drive productivity, instead of relying on a handful of large firms for essential technology. Finally, open-source AI promotes security. Public and private sector organizations can adapt open-source models for specialized applications without sharing private or sensitive data with a proprietary firm”\nThe rest of this blog post concerns itself primarily with the second point raised by LAION which, in my view, is the primary reason current Big Tech AI incumbents are pushing so heavily for regulation. I’m not saying attempted regulatory capture to erect barriers to entry and crush competition but…\nBig Tech, Billionaires and Whole Lot of Lobbying In the United States, Artificial Intelligence related lobbying has spiked dramatically in the past two years. As reported in CNBC, AI lobbying efforts spiked 185% from 2022 to 2023 with over 450 organizations participating in efforts to influence US Federal legislation. This spike corresponded with growing calls for AI regulation and the Biden administration’s push to codify such rules into law. A whole host of corporations and industries ranging from the expected Big Tech and AI to pharmaceuticals, insurance, finance, telecommunications and data brokerages are involved in these efforts. Even Disney is splashing their cash on the AI scene. As of February 2024 these entities have spent in excess of $950 million in lobbying efforts. But that is not all.\nThere are billionaire-backed networks of AI advisers that have “taken over Washington” as Politico so succinctly puts it. These networks are “spread across Congress, federal agencies and think tanks” with one of them essentially funneling Big Tech money “through a science nonprofit to help pay the salaries of AI staffers in Congress.” This blog post highlights two of these networks, however it must be acknowledged these are simply just two good examples of the increasing influence Big Tech is having on AI policy within the US Federal government.\nThe first organization of note is Open Philanthropy, primarily funded by Dustin Moskovitz: billionaire Facebook co-founder and CEO of Asana who also happens to be among the Biden campaign’s biggest donors. Through the Horizon Institute for Public Service, Open Philanthropy funds the salaries of key individuals working in key bodies responsible for AI rule making:\n“Current and former Horizon AI fellows with salaries funded by Open Philanthropy are now working at the Department of Defense, the Department of Homeland Security and the State Department, as well as in the House Science Committee and Senate Commerce Committee, two crucial bodies in the development of AI rules. They also populate key think tanks shaping AI policy, including the RAND Corporation and Georgetown University’s Center for Security and Emerging Technology, according to the Horizon web site. In 2022, Open Philanthropy set aside nearly $3 million to pay for what ultimately became the initial cohort of Horizon fellows.” - How a billionaire-backed network of AI advisers took over Washington, POLITICO\nThe second group of note is a “rapid response cohort” of AI fellows (lobbyists) responsible for supporting “leaders in Congress as they craft legislation, in particular policies related to emerging opportunities and challenges with AI.” Run by the American Association for the Advancement of Science with “substantial support from Microsoft, ‘Open’AI, Google, IBM and Nvidia,” this cohort of lobbyists exerts considerable influence on Congressional AI rule making.\n“Alongside the Open Philanthropy fellows — and hundreds of outside-funded fellows throughout the government, including many with links to the tech industry — the six AI staffers in the industry-funded rapid response cohort are helping shape how key players in Congress approach the debate over when and how to regulate AI, at a time when many Americans are deeply skeptical of the industry.” - Key Congress staffers in AI debate are funded by tech giants like Google and Microsoft, POLITICO\nThis immense lobbying effort is not limited to Washington, though due to the United State’s political system this is perhaps the best showcase of Big Tech’s influence in action.\nAcross the Atlantic, Rishi Sunak’s government, at first nonchalant about the increasing usage and development of AI has had the dangers of “existential risk pushed right up on the policy agenda” by key government advisors linked heavily to AI companies and Big Tech.\nIn the European Union, the recently passed “AI Act” was heavily lobbied by Big Tech. Lobbying efforts, such as those by ‘OpenAI’, were focused on watering down the ways in which the law would burden the company. In multiple cases, \"‘Open’AI proposed amendments that were later made to the final text of the EU law.\" This is in seeming direct contradiction with the statements released by these companies about the need for regulation. Hypocrisy from Big Tech and their executives? Who would have thought?\n“What they’re saying is basically: trust us to self-regulate,” says Daniel Leufer, a senior policy analyst focused on AI at Access Now’s Brussels office. “It’s very confusing because they’re talking to politicians saying, ‘Please regulate us,’ they’re boasting about all the safety stuff that they do, but as soon as you say, ‘Well, let’s take you at your word and set that as a regulatory floor,’ they say no.” - OpenAI Lobbied the E.U. to Water Down AI Regulation, TIME It is undeniable that there exists considerable influence from Big Tech in Artificial Intelligence regulation and policy. Regardless of your views on these companies, history is littered with examples of large powerful corporations, Big Tech or otherwise, pursuing their profit incentive through legislative capture at the expense of common good. Why should these companies act any different?\n“Tim Stretton, director of the congressional oversight initiative at the nonpartisan watchdog Project On Government Oversight, said it’s “never great when corporations are funding, essentially, congressional staffers.” He said the money from five leading AI firms, suggests an improper level of tech industry influence.” - Key Congress staffers in AI debate are funded by tech giants like Google and Microsoft, POLITICO\nSen. Dick Durbin (D., Ill.) remarked that he could not recall a time when representatives for private sector entities had ever pleaded for regulation. - OpenAI CEO Sam Altman Asks Congress to Regulate AI, TIME\nEffective Altruism and the Focus on Existential Threats The intense lobbying efforts on the part of Big Tech to influence the AI regulatory agenda appear to be heavily linked to “effective altruism” (EA): the controversial Silicon Valley ideology and movement that, amongst other things seems to “advocate policy that’s focused on the distant future rather than the here-and-now.” According to industry insiders the ideology is now “driving the research agenda in the field of artificial intelligence (AI), creating a race to proliferate harmful systems, ironically in the name of “AI safety.”\n“Some of the billionaires who have committed significant funds to this goal include Elon Musk, Vitalik Buterin, Ben Delo, Jaan Tallinn, Peter Thiel, Dustin Muskovitz, and Sam Bankman-Fried, who was one of EA’s largest funders until the recent bankruptcy of his FTX cryptocurrency platform. As a result, all of this money has shaped the field of AI and its priorities in ways that harm people in marginalized groups while purporting to work on “beneficial artificial general intelligence” that will bring techno utopia for humanity. This is yet another example of how our technological future is not a linear march toward progress but one that is determined by those who have the money and influence to control it.” - Effective Altruism Is Pushing a Dangerous Brand of ‘AI Safety’ The above mentioned organizations, such as Open Philanthropy, have significant ties the movement, with many AI thinkers seeing the existential AI threats that EA proponents push being “science-fiction concerns far removed from the current AI harms” that should be addressed, resulting in the “steering of policy conversation away from more pressing issues - including topics some leading AI firms might prefer to keep off the policy agenda.”\n“The network’s fixation on speculative harms is “almost like a caricature of the reality that we’re experiencing,” said Deborah Raji, an AI researcher at the University of California, Berkeley, who attended last month’s AI Insight Forum in the Senate. She worries that the focus on existential dangers will steer lawmakers away from addressing risks that today’s AI systems already pose, including their tendency to inject bias, spread misinformation, threaten copyright protections and weaken personal privacy.” - How a billionaire-backed network of AI advisers took over Washington, POLITICO\nExtensively Lobbied Regulations Benefit Incumbents The Effects of Such Lobbying With the influence of Big Tech AI incumbents and the ideas of ’existential threats’ evidently having a massive role to play in the drafting of AI regulation, one could expect policy proposals to benefit the interests of such parties.\nSo lets have a look at an example of a legislative proposal: namely one bipartisan bill proposed in the United States by U.S. Senators Richard Blumenthal (D-CT) and Josh Hawley (R-MO), Chair and Ranking Member of the Senate Judiciary Subcommittee on Privacy, Technology, and the Law. Within this proposed legislative framework the key takeaways are:\n“The establishment of a licencing regime administered by an independent oversight body” which AI developers working on sophisticated general purpose models would be required to register with the body. In this context a licencing scheme is just another word for permission. The above mentioned body would “ensure legal accountability for harms” through the requirement of AI companies being held liable when harms are caused through the use of their models. This legislation echoes the type of regulation called for by the likes of ‘Open’AI in the May 2023 Congressional hearing on AI.\n“he (Sam Altman) supported the creation of a federal agency that can grant licenses to create AI models above a certain threshold of capabilities, and can also revoke those licenses if the models don’t meet safety guidelines set by the government.” - OpenAI CEO Sam Altman Asks Congress to Regulate AI, TIME\nThis establishment of a strict liability regime based upon existential harms caused by AI models would ensure that only large, already established incumbent companies would have the financial and technical ability to comply with the law, while new startups and alternative non corporate structures (such as open-source) would face serious barriers to entry. Such a legislative proposal, if passed into law, would almost certainly pose limits, if not entirely restrict the legal development of open-source AI while leaving only the big, closed and proprietary models standing.\nOpposition to the Current Regulatory Narrative This is a view shared by a growing number of key AI figures, academics and technologists. Perhaps the most notable of these is Andrew Ng: Stanford Professor; machine learning teacher to ‘Open’AI CEO Sam Altman; co-founder of Google Brain and a globally recognized leader in Artificial Intelligence. He is of the belief that notions of Artificial Intelligence leading to the extinction of the human race is “a lie being promulgated by big tech in the hope of triggering heavy regulation that would shut down competition in the AI market” and a big proponent of open-source AI development.\n“Andrew Ng said that the “bad idea that AI could make us go extinct” was merging with the “bad idea that a good way to make AI safer is to impose burdensome licensing requirements” on the AI industry.” - Google Brain founder says big tech is lying about AI extinction danger, Financial Review I would highly advise a read of the interview with Andrew Ng from the Financial Times, of which I will take key snippets. (Extracts copied below are for nonprofit educational purposes under Section 107 of the Copyright Act: Fair Use and Article 5: Information Society Directive)\n“Open-source software’s getting easy enough for most people to just install it and use it now. And it’s not that I’m obsessed about regulation — but if some of the regulators have their way, it’d be much harder to let open-source models like this keep up.”\n“Some proposals, for instance, have reporting or even licensing requirements for LLMs. And while the big tech companies have the bandwidth to deal with complex compliance, smaller businesses just don’t.”\n“When I think about the AI human extinction scenarios, when I speak with people who say they’re concerned about this, their concerns seem very vague. And no one seems to be able to articulate exactly how AI could kill us all.”\n“There is also some chance that is absolutely non-zero of our radio signals causing aliens to find us and wipe us all out. But the chance is so small that we should not waste disproportionate resources to defend against that danger. And what I’m seeing is that we are spending vastly disproportionate resources against a risk that is almost zero.”\n“Multiple companies are overhyping the threat narrative. For large businesses that would rather not compete with open-source, there is an incentive. For some non-profits, there is an incentive to hype up fears, hype up phantoms, and then raise funding to fight the phantoms they themselves conjured. And there are also some individuals who are definitely commanding more attention and larger speaker fees because of fear that they are helping to hype up. I think there are a few people who are sincere — mistaken but sincere — but on top of that there are significant financial incentives for one or multiple parties to hype up fear.”\n“When lots of people signed [the Center for AI Safety statement] saying AI is dangerous like nuclear weapons, the media covered that. When there have been much more sensible statements — for example, Mozilla saying that open source is a great way to ensure AI safety — almost none of the media cover that.”\nAndrew Ng is not alone in these views. In addition to the LAION open letter addressed to the European Parliament calling for the protection of open-source AI that was referred to previously, Mozilla has also released a Joint Statement on AI Safety and Openness signed by a varied range of companies, civil society organisations and leading individuals in the realms of computer science, engineering, journalism and policy making.\n“Yes, openly available models come with risks and vulnerabilities — AI models can be abused by malicious actors or deployed by ill-equipped developers. However, we have seen time and time again that the same holds true for proprietary technologies — and that increasing public access and scrutiny makes technology safer, not more dangerous. The idea that tight and proprietary control of foundational AI models is the only path to protecting us from society-scale harm is naive at best, dangerous at worst.” - Joint Statement on AI Safety and Openness, Mozilla\n“Further, history shows us that quickly rushing towards the wrong kind of regulation can lead to concentrations of power in ways that hurt competition and innovation. Open models can inform an open debate and improve policy making. If our objectives are safety, security and accountability, then openness and transparency are essential ingredients to get us there.” - Joint Statement on AI Safety and Openness, Mozilla\nIn addition to this, there is growing research highlighting the concerns of Big Tech influenced regulation on open-source AI development and the resultant negative impacts upon competition such as this piece by the Carnegie Endowment:\n“policy measures that address only speculative superintelligence concerns (and not more evolutionary AI policy challenges) are especially likely to impose steep costs in exchange for minimal benefits.” - How Hype Over AI Superintelligence Could Lead Policy Astray, Carnegie Endowment and the peer reviewed paper from Stanford in the George Washington Law Review: “AI Regulation Has Its Own Alignment Problem: The Technical and Institutional Feasibility of Disclosure, Registration, Licensing, and Auditing”:\n“the potential of licensing to undermine competition, raise costs to consumers, enable industry capture, and gatekeep professions indicates AI licensing would create horizontal misalignment… AI licensing that places significant pre- and post-market burdens on companies may be prohibitively costly for smaller developers.”\n“Licensing the development or deployment of AI thus has the potential to concentrate economic power in the hands of a few large companies… Licensing may heighten market concentration by advantaging more established incumbents who can more easily bear the licensing costs.”\n“Concentration of market power could even exacerbate other harms arising from AI or undermine human values and regulatory objectives these policies aim to promote.”\n“Licensing also creates tradeoffs between openness and control. Open access may provide for less control by enabling individuals with bad intentions or insufficient training to more easily access resources, but it may also increase the likelihood that critical issues with the technology are identified after release. Licensing may make it harder for users to expose harms, especially considering how openness provided mechanisms for discovering and addressing cybersecurity risks.”\n“licensing regimes are particularly susceptible to capture. For example, research suggests that lobbying by physician interest groups is linked to a higher probability that a state will have occupational licensing in the healthcare industry. The potential for special interest groups to have outsized impact on AI licensing regimes is particularly worrisome given licensing may make the frontier of AI technology inaccessible to most.”\nThese views have not gone unnoticed. The European Union’s landmark AI Act has carve outs and exceptions for open-source AI development. Only time will tell how far these exceptions go.\nGiven leaked documentation from a Google engineer that warns it and ‘Open’AI could lose out to open-source technology in the ‘AI arms race’, it is clear that Big Tech is concerned with the prospects of widely available open AI and is evidently lobbying hard for rules to be crafted to their benefit as an industry.\nLaw Enforcement and the National Security State is Getting their Way as Well While not the main focus of this post, it is worthwhile to note that it is not just Big Tech and the AI industry that is trying to get/ getting their way when it comes to AI regulation and policy. Law enforcement and national security agencies, leaders and policy makers are also involved in setting the agenda. There is a clear revolving door and incestuous relationship between the National Security State and the Big Tech Surveillance Capitalists, best exemplified by ‘Open’AI’s recent appointment of former NSA Director and spook in chief Paul M. Nakasone to the position of the company’s board and the ‘Safety and Security Committee’.\nIn the EU, civil society organizations have raised the alarm about vague terms, exceptions and carve outs that the AI Act affords to law enforcement and national security agencies allowing them to use AI and conduct biometric and facial recognition on a mass societal level. Some of these statements are linked below:\nEU’s AI Act fails to set gold standard for human rights, European Digital Rights Packed with loopholes: Why the AI Act fails to protect civic space and the rule of law, European Digital Rights and the European Center for Not-for-Profit Law EU lawmakers must regulate the harmful use of tech by law enforcement in the AI Act, European Digital Rights The EU AI Act: a failure for human rights, a victory for industry and law enforcement, Accessnow WITH THE AI ACT ADOPTED, THE TECHNO-SOLUTIONIST GOLD-RUSH CAN CONTINUE, La Quadrature du Net Conclusions There are clearly risks posed by the widespread adoption of Artificial Intelligence and in no way am I diminishing the work to address these. Risks posed by AI in the here and now are varied but very real. These include but are not limited to:\nOnline political influence operations using AI generated content masquerading as genuine human created content to proliferate disinformation and push agendas. Privacy concerns arising out of the usage of AI systems to sort through, link together and synthesize vast amounts of data to create detailed social scores and profiles of individuals. Privacy concerns arising out of the usage of AI in the realm of surveillance, facial recognition and biometric identification and the risks of bias and abuse. Concerns with Artificial Intelligence’s application in armed conflict, regarding accountability and compliance with the legal obligations of war. Concerns with AI in policing and national security matters, particularly ‘predictive’ models. The usage of AI agents to wage cyber wars, and attack critical infrastructure through cyber attacks. The usage of AI in the commission of cyber crimes ranging from the generation of lifelike child sexual abuse material to AI enhanced phishing and hacks. I am no expert on matters relating to AI and I do not claim to be. What I will claim to have, however, is extensive knowledge on the current surveillance society we all reside in, where we are all subject to the invisible panopticon brought about by the same mega corporations and State agencies (the Surveillance Capitalist - National Security State nexus) that is now developing AI and extensively influencing and guiding the regulation of such technologies. Forgive me if I have a large degree of skepticism towards these companies and institutions and their motivations. I think the record speaks for itself on how they behave and operate. If you think it does not I suggest taking a look at the readings under the Privacy and Security Resources section of my website.\nIt is my view that ethical and regulatory considerations towards AI systems are not wholly unjustified and in many cases are certainly valid. However, they can, have been and will be utilized to impose limitations designed primarily to concentrate power over such systems in the hands of those who are in the position to benefit from their usage the most. The Surveillance Capitalist - National Security State institutional nexus does not want ordinary people to be able to deploy locally run and open-source AI’s, as in such a scenario it would not be capable of extracting people’s money, data and behavioral surplus nor retain their monopolistic grip on the digital domain.\nThe suggested risks posed by open AI systems and proposals to establish regulatory regimes and oversight bodies complete with licencing (permission) and strict liability schemes that will crush open AI development are purported to be for AI safety and the benefit of society. But this obscures the reality. The above mentioned nexus is not to be trusted, with them pretending to serve the common good under a guise of morality as they pursue their own self interests of profit and informational power, regardless of consequences for the public. This is what I believe to be readily transparent and clear.\nThis is also raises another question. If Artificial Intelligence systems are indeed an existential threat and far too dangerous for them to be open sourced, democratized and available to the public at large then on what grounds is it not likewise too dangerous for such power to be consolidated in the hand of Big Tech, the Surveillance Capitalists and the National Security State apparatus?\nIt is my opinion that the ultimate threat posed by AI is not some doomsday extinction event but rather the consolidation of Artificial Intelligence power in the hands of an exclusive high ‘priesthood’, without true accountability and transparency. We have seen what the internet looks like today; a locked down, commodified shell of its original aspirations that places us all as economic objects in the relentless drive to extract and monetize ever more behavioral surplus, that is Surveillance Capitalism, while we are all being watched under the omniscient eye of Big Brother(s). Artificial Intelligence has the potential to up end this relationship between us and informational power centers or to forever render us as informational commodities. So which will it be? Only time will tell but that future rests upon whether or not AI power is consolidated or dispersed within society.\n“The liberty of a democracy is not safe if the people tolerate the growth of private power to a point where it becomes stronger than their democratic state itself. That, in its essence, is Fascism—ownership of Government by an individual, by a group, or by any other controlling private power.” - Franklin D. Roosevelt, 32nd President of the United States of America\nDisclaimer: I do not, by any means, claim to be an expert in matters relating to privacy, security and law or offer what can be construed as guaranteed fool-proof advice. What I do offer is an insight into these matters from someone who is highly invested in personal privacy/ security themselves and who is studying technology law at the level of higher education.\n",
  "wordCount" : "4517",
  "inLanguage": "en",
  "image":"http://localhost:1313/images/eyewatchingmandesk.jpeg/","datePublished": "2024-06-29T00:00:00Z",
  "dateModified": "2024-06-29T00:00:00Z",
  "author":{
    "@type": "Person",
    "name": "Luke Arbuthnot"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "http://localhost:1313/posts/bigtechandai/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Privacy, Security and the Law",
    "logo": {
      "@type": "ImageObject",
      "url": "http://localhost:1313/favicon.ico"
    }
  }
}
</script>
</head>

<body class=" dark" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="http://localhost:1313/" accesskey="h" title="PrivSecLaw (Alt + H)">
                <img src="http://localhost:1313/privseclaw.png" alt="" aria-label="logo"
                    height="30">PrivSecLaw</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="http://localhost:1313/posts/" title="Blog">
                    <span>Blog</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/resources/" title="Resources">
                    <span>Resources</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/aboutme/" title="About Me">
                    <span>About Me</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="http://localhost:1313/">Home</a>&nbsp;»&nbsp;<a href="http://localhost:1313/posts/">Privacy, Security and the Law</a></div>
    <h1 class="post-title entry-hint-parent">
      The AI Panic: Big Tech, Open-source and Monopoly Power
    </h1>
    <div class="post-meta"><span title='2024-06-29 00:00:00 +0000 UTC'>June 29, 2024</span>&nbsp;·&nbsp;22 min&nbsp;·&nbsp;4517 words&nbsp;·&nbsp;Luke Arbuthnot

</div>
  </header> 
<figure class="entry-cover"><img loading="eager" src="http://localhost:1313/images/eyewatchingmandesk.jpeg/" alt="eyewatchingmanatdesk">
        
</figure><div class="toc">
    <details >
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><ul>
                <li>
                    <a href="#the-existential-threat-of-ai" aria-label="The Existential Threat of AI?">The Existential Threat of AI?</a></li>
                <li>
                    <a href="#two-visions-of-the-future-of-ai" aria-label="Two Visions of the Future of AI">Two Visions of the Future of AI</a></li>
                <li>
                    <a href="#big-tech-billionaires-and-whole-lot-of-lobbying" aria-label="Big Tech, Billionaires and Whole Lot of Lobbying">Big Tech, Billionaires and Whole Lot of Lobbying</a></li>
                <li>
                    <a href="#effective-altruism-and-the-focus-on-existential-threats" aria-label="Effective Altruism and the Focus on Existential Threats">Effective Altruism and the Focus on Existential Threats</a></li>
                <li>
                    <a href="#extensively-lobbied-regulations-benefit-incumbents" aria-label="Extensively Lobbied Regulations Benefit Incumbents">Extensively Lobbied Regulations Benefit Incumbents</a><ul>
                        
                <li>
                    <a href="#the-effects-of-such-lobbying" aria-label="The Effects of Such Lobbying">The Effects of Such Lobbying</a></li>
                <li>
                    <a href="#opposition-to-the-current-regulatory-narrative" aria-label="Opposition to the Current Regulatory Narrative">Opposition to the Current Regulatory Narrative</a></li>
                <li>
                    <a href="#law-enforcement-and-the-national-security-state-is-getting-their-way-as-well" aria-label="Law Enforcement and the National Security State is Getting their Way as Well">Law Enforcement and the National Security State is Getting their Way as Well</a></li></ul>
                </li>
                <li>
                    <a href="#conclusions" aria-label="Conclusions">Conclusions</a>
                </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><h3 id="the-existential-threat-of-ai">The Existential Threat of AI?<a hidden class="anchor" aria-hidden="true" href="#the-existential-threat-of-ai">#</a></h3>
<p>There is currently a concerted effort to portray Artificial Intelligence developments as an <a href="https://www.wired.com/story/runaway-ai-extinction-statement/">extinction level risk to humanity, on par with a nuclear war and deadly pandemics.</a> AI experts, namely <a href="https://www.safe.ai/work/statement-on-ai-risk">leaders and CEO&rsquo;s from Big Tech and adjacent industries, certain academics</a> and billionaire backed philanthropy groups have all called for <a href="https://time.com/6280372/sam-altman-chatgpt-regulate-ai/">state intervention and regulation of their own industry.</a>  The mass media, quick to pick up on claims of the oncoming AI apocalypse, have regurgitated these claims from the experts, drumming up popular apprehension amongst the general public, elected officials and policy makers.</p>
<blockquote>
<p><em>&ldquo;My worst fears are that we—the field, the technology, the industry—cause significant harm to the world. I think that can happen in a lot of different ways&rdquo;</em> - <strong><a href="https://time.com/6280372/sam-altman-chatgpt-regulate-ai/">Sam Altman, &lsquo;Open&rsquo;AI</a></strong></p>
</blockquote>
<blockquote>
<p><em>&ldquo;If somebody builds a too-powerful AI, under present conditions, I expect that every single member of the human species and all biological life on Earth dies shortly thereafter&rdquo;</em> - <strong><a href="https://time.com/6266923/ai-eliezer-yudkowsky-open-letter-not-enough/">Eliezer Yudkowsky, Machine Intelligence Research Institute</a></strong></p>
</blockquote>
<p>Looking at these presented grave dangers from industry insiders and leaders it appears to me that something does not add up. AI is purported to have the potential to be catastrophic for the human race yet AI development has not been paused. Quite to the contrary it has <a href="https://www.cnbc.com/2024/02/09/openai-ceo-sam-altman-reportedly-seeking-trillions-of-dollars-for-ai-chip-project.html">accelerated at breakneck pace</a> as for-profit, Big Tech companies seek to cement and <a href="https://www.bloomberg.com/company/press/generative-ai-to-become-a-1-3-trillion-market-by-2032-research-finds/">expand</a> their market positions and develop ever more powerful AI models.</p>
<h3 id="two-visions-of-the-future-of-ai">Two Visions of the Future of AI<a hidden class="anchor" aria-hidden="true" href="#two-visions-of-the-future-of-ai">#</a></h3>
<p>There is a conflict playing out as to how AI is to be developed.</p>
<p>On one side there is the nexus of Big Tech, the <a href="https://www.youtube.com/watch?v=4oOCLIrB-0c">Surveillance Capitalists</a> and increasingly elements of <a href="https://www.theverge.com/2024/6/13/24178079/openai-board-paul-nakasone-nsa-safety">immense State power</a> which, as put eloquently by Zuboff, <a href="https://journals.sagepub.com/doi/10.1177/26317877221129290">&ldquo;constitute a sweeping political-economic institutional order that exerts oligopolistic control over most digital information and communication spaces, systems, and processes.&rdquo;</a> These entities would fashion themselves as a technological high priesthood, <a href="https://analyticsindiamag.com/openai-raises-alarm-over-open-source-ai-dangers/">gate-keeping the development</a> and <a href="https://openai.com/index/reimagining-secure-infrastructure-for-advanced-ai/">deployment</a> of AI as regulated, restricted and walled off behind their proprietary, nontransparent systems. This is not new. <a href="https://web.archive.org/web/20240105020753/https://www.cnet.com/tech/services-and-software/dead-and-buried-microsofts-holy-war-on-open-source-software/">Large technology corporations</a> and <a href="https://wiki.openrightsgroup.org/wiki/Crypto_Wars">States</a> have historically been opposed to open source and democratized technology as it challenges their institutional and monopoly powers within society.</p>
<p>On the other side, there are a growing number of individuals, companies (including some surprises such as <a href="https://www.theguardian.com/technology/2023/dec/05/open-source-ai-meta-ibm">Meta and IBM</a>) and parts of <a href="https://open.mozilla.org/letter/">civil society</a> that reject this view and see open sourced Artificial Intelligence as necessary for innovation and safety through transparency, effective competition and security. As seen with <a href="https://laion.ai/">LAION&rsquo;s</a> <a href="https://laion.ai/notes/letter-to-the-eu-parliament/">&ldquo;Call to Protect Open-Source AI in Europe,&rdquo;</a> the touted benefits of open-source AI are security, competition and safety:</p>
<blockquote>
<p><em>&ldquo;First, <strong>open-source AI promotes safety through transparency</strong>. Open-sourcing data, models, and workflows enables researchers and authorities to audit the performance of a model or system; develop interpretability techniques; identify risks; and establish mitigations or develop anticipatory countermeasures. Second, <strong>open-source AI promotes competition</strong>. Small to medium enterprises across Europe can build on open-source models to drive productivity, instead of relying on a handful of large firms for essential technology. Finally, <strong>open-source AI promotes security</strong>. Public and private sector organizations can adapt open-source models for specialized applications without sharing private or sensitive data with a proprietary firm&rdquo;</em></p>
</blockquote>
<p>The rest of this blog post concerns itself primarily with the second point raised by LAION which, in my view, is the primary reason current Big Tech AI incumbents are pushing so heavily for regulation. I&rsquo;m not saying attempted regulatory capture to erect barriers to entry and crush competition but&hellip;</p>
<h3 id="big-tech-billionaires-and-whole-lot-of-lobbying">Big Tech, Billionaires and Whole Lot of Lobbying<a hidden class="anchor" aria-hidden="true" href="#big-tech-billionaires-and-whole-lot-of-lobbying">#</a></h3>
<p>In the United States, Artificial Intelligence related lobbying has spiked dramatically in the past two years. As reported in <a href="https://www.cnbc.com/2024/02/02/ai-lobbying-spikes-nearly-200percent-as-calls-for-regulation-surge.html">CNBC</a>, AI lobbying efforts spiked 185% from 2022 to 2023 with over 450 organizations participating in efforts to influence US Federal legislation. This spike corresponded with growing calls for AI regulation and the Biden administration&rsquo;s push to codify such rules into law. A whole host of corporations and industries ranging from the expected Big Tech and AI to pharmaceuticals, insurance, finance, telecommunications and data brokerages are involved in these efforts. Even Disney is splashing their cash on the AI scene. As of February 2024 these entities have spent in excess of $950 million in lobbying efforts. But that is not all.</p>
<p>There are billionaire-backed networks of AI advisers that have &ldquo;taken over Washington&rdquo; as <a href="https://www.politico.com/news/2023/10/13/open-philanthropy-funding-ai-policy-00121362">Politico</a> so succinctly puts it. These networks are &ldquo;spread across Congress, federal agencies and think tanks&rdquo; with one of them essentially funneling Big Tech money <a href="https://www.politico.com/news/2023/12/03/congress-ai-fellows-tech-companies-00129701">&ldquo;through a science nonprofit to help pay the salaries of AI staffers in Congress.&rdquo;</a> This blog post highlights two of these networks, however it must be acknowledged these are simply just two good examples of the increasing influence Big Tech is having on AI policy within the US Federal government.</p>
<p>The first organization of note is Open Philanthropy, primarily funded by <a href="https://www.openphilanthropy.org/about-us/">Dustin Moskovitz:</a> billionaire Facebook co-founder and CEO of Asana who also happens to be among the Biden campaign&rsquo;s <a href="https://www.opensecrets.org/2020-presidential-race/joe-biden/contributors?id=N00001669">biggest donors</a>. Through the <a href="https://horizonpublicservice.org/about-us/">Horizon Institute for Public Service</a>, Open Philanthropy funds the salaries of key individuals working in key bodies responsible for AI rule making:</p>
<blockquote>
<p><em>&ldquo;Current and former Horizon AI fellows with salaries funded by Open Philanthropy are now working at the Department of Defense, the Department of Homeland Security and the State Department, as well as in the House Science Committee and Senate Commerce Committee, two crucial bodies in the development of AI rules. They also populate key think tanks shaping AI policy, including the RAND Corporation and Georgetown University’s Center for Security and Emerging Technology, according to the Horizon web site. In 2022, Open Philanthropy <a href="https://www.openphilanthropy.org/grants/open-philanthropy-technology-policy-fellowship-2022/">set aside nearly $3 million to pay for</a> what ultimately became the initial cohort of Horizon fellows.&rdquo;</em> - <a href="https://www.politico.com/news/2023/10/13/open-philanthropy-funding-ai-policy-00121362"><strong>How a billionaire-backed network of AI advisers took over Washington, POLITICO</strong></a></p>
</blockquote>
<p>The second group of note is a <a href="https://www.aaas.org/news/stpf-ai-cohort">&ldquo;rapid response cohort&rdquo;</a> of AI fellows (lobbyists) responsible for supporting &ldquo;leaders in Congress as they craft legislation, in particular policies related to emerging opportunities and challenges with AI.&rdquo; Run by the American Association for the Advancement of Science with <a href="https://www.politico.com/news/2023/12/03/congress-ai-fellows-tech-companies-00129701">&ldquo;substantial support from Microsoft, &lsquo;Open&rsquo;AI, Google, IBM and Nvidia,&rdquo; </a> this cohort of lobbyists exerts considerable influence on Congressional AI rule making.</p>
<blockquote>
<p><em>&ldquo;Alongside the Open Philanthropy fellows — and hundreds of outside-funded fellows throughout the government, including many with links to the tech industry — the six AI staffers in the industry-funded rapid response cohort are helping shape how key players in Congress approach the debate over when and how to regulate AI, at a time when many Americans are deeply skeptical of the industry.&rdquo;</em> - <a href="https://www.politico.com/news/2023/12/03/congress-ai-fellows-tech-companies-00129701"><strong>Key Congress staffers in AI debate are funded by tech giants like Google and Microsoft, POLITICO</strong></a></p>
</blockquote>
<p>This immense lobbying effort is not limited to Washington, though due to the United State&rsquo;s political system this is perhaps the best showcase of Big Tech&rsquo;s influence in action.</p>
<p>Across the Atlantic, Rishi Sunak&rsquo;s government, at first nonchalant about the increasing usage and development of AI has had the dangers of <a href="https://www.politico.eu/article/rishi-sunak-artificial-intelligence-pivot-safety-summit-united-kingdom-silicon-valley-effective-altruism/">&ldquo;existential risk pushed right up on the policy agenda&rdquo;</a> by key government advisors linked heavily to AI companies and Big Tech.</p>
<p>In the European Union, the recently passed <a href="https://artificialintelligenceact.eu/">&ldquo;AI Act&rdquo;</a> was heavily lobbied by <a href="https://time.com/6273694/ai-regulation-europe/">Big Tech.</a> Lobbying efforts, such as those by &lsquo;OpenAI&rsquo;, were focused on watering down the ways in which the law would burden the company. In multiple cases, <a href="https://time.com/6288245/openai-eu-lobbying-ai-act/">&quot;&lsquo;Open&rsquo;AI proposed amendments that were later made to the final text of the EU law.&quot;</a> This is in seeming direct contradiction with the statements released by these companies about the need for regulation. Hypocrisy from Big Tech and their executives? Who would have thought?</p>
<blockquote>
<p><em>“What they’re saying is basically: trust us to self-regulate,” says Daniel Leufer, a senior policy analyst focused on AI at Access Now’s Brussels office. “It’s very confusing because they’re talking to politicians saying, ‘Please regulate us,’ they’re boasting about all the safety stuff that they do, but as soon as you say, ‘Well, let’s take you at your word and set that as a regulatory floor,’ they say no.”</em> - <a href="https://time.com/6288245/openai-eu-lobbying-ai-act/"><strong>OpenAI Lobbied the E.U. to Water Down AI Regulation, TIME</strong> </a></p>
</blockquote>
<p>It is undeniable that there exists considerable influence from Big Tech in Artificial Intelligence regulation and policy. Regardless of your views on these companies, history is littered with examples of large powerful corporations, Big Tech or otherwise, pursuing their profit incentive through legislative capture at the expense of common good. Why should these companies act any different?</p>
<blockquote>
<p><em>&ldquo;Tim Stretton, director of the congressional oversight initiative at the nonpartisan watchdog Project On Government Oversight, said it’s “never great when corporations are funding, essentially, congressional staffers.” He said the money from five leading AI firms, suggests an improper level of tech industry influence.&rdquo;</em> - <a href="https://www.politico.com/news/2023/12/03/congress-ai-fellows-tech-companies-00129701"><strong>Key Congress staffers in AI debate are funded by tech giants like Google and Microsoft, POLITICO</strong></a></p>
</blockquote>
<blockquote>
<p>Sen. Dick Durbin (D., Ill.) remarked that he could not recall a time when representatives for private sector entities had ever pleaded for regulation. - <strong><a href="https://time.com/6280372/sam-altman-chatgpt-regulate-ai/">OpenAI CEO Sam Altman Asks Congress to Regulate AI, TIME</a></strong></p>
</blockquote>
<h3 id="effective-altruism-and-the-focus-on-existential-threats">Effective Altruism and the Focus on Existential Threats<a hidden class="anchor" aria-hidden="true" href="#effective-altruism-and-the-focus-on-existential-threats">#</a></h3>
<p>The intense lobbying efforts on the part of Big Tech to influence the AI regulatory agenda appear to be heavily linked to &ldquo;effective altruism&rdquo; (EA): the controversial Silicon Valley ideology and movement that, amongst other things seems to <a href="https://www.politico.eu/article/rishi-sunak-artificial-intelligence-pivot-safety-summit-united-kingdom-silicon-valley-effective-altruism/">&ldquo;advocate policy that’s focused on the distant future rather than the here-and-now.&rdquo;</a> According to industry insiders the ideology is now <a href="https://web.archive.org/web/20240108214418/https://www.wired.com/story/effective-altruism-artificial-intelligence-sam-bankman-fried/">&ldquo;driving the research agenda in the field of artificial intelligence (AI), creating a race to proliferate harmful systems, ironically in the name of “AI safety.&rdquo;</a></p>
<blockquote>
<p><em>&ldquo;Some of the billionaires who have committed significant funds to this goal include <a href="https://web.archive.org/web/20240108214418/https://twitter.com/elonmusk/status/495759307346952192">Elon Musk</a>, <a href="https://web.archive.org/web/20240108214418/https://forum.effectivealtruism.org/posts/wFC3axfuwABHmoQ9H/the-vitalik-buterin-fellowship-in-ai-existential-safety-is">Vitalik Buterin</a>, <a href="https://web.archive.org/web/20240108214418/https://www.openphilanthropy.org/research/co-funding-partnership-with-ben-delo/">Ben Delo</a>, <a href="https://web.archive.org/web/20240108214418/https://www.cser.ac.uk/team/jaan-tallinn/">Jaan Tallinn</a>, <a href="https://web.archive.org/web/20240108214418/https://www.ft.com/content/abc942cc-5fb3-11e4-8c27-00144feabdc0">Peter Thiel</a>, <a href="https://web.archive.org/web/20240108214418/https://www.ft.com/content/15ffce3b-cee3-44ad-b961-e22459b7b7b2">Dustin Muskovitz</a>, and <a href="https://web.archive.org/web/20240108214418/https://inside.com/ai/posts/anthropic-raises-580m-from-bankman-fried-tallinn-for-safer-ai-282609">Sam Bankman-Fried</a>, who was one of <a href="https://web.archive.org/web/20240108214418/https://www.washingtonpost.com/technology/2022/11/17/effective-altruism-sam-bankman-fried-ftx-crypto/">EA’s largest funders</a> until the recent bankruptcy of his FTX cryptocurrency platform. As a result, all of this money has shaped the field of AI and its priorities in ways that harm people in marginalized groups while purporting to work on “beneficial artificial general intelligence” that will bring techno utopia for humanity. This is yet another example of how our technological future is not a linear march toward progress but one that is determined by those who have the money and influence to control it.&rdquo;</em> - <a href="https://web.archive.org/web/20240108214418/https://www.wired.com/story/effective-altruism-artificial-intelligence-sam-bankman-fried/"><strong>Effective Altruism Is Pushing a Dangerous Brand of ‘AI Safety’</strong> </a></p>
</blockquote>
<p>The above mentioned organizations, such as Open Philanthropy, have <a href="https://www.politico.com/news/2023/10/13/open-philanthropy-funding-ai-policy-00121362">significant ties the movement</a>, with many AI thinkers seeing the existential AI threats that EA proponents push being &ldquo;science-fiction concerns far removed from the current AI harms&rdquo; that should be addressed, resulting in the &ldquo;steering of policy conversation away from more pressing issues - including topics some leading AI firms might prefer to keep off the policy agenda.&rdquo;</p>
<blockquote>
<p><em>&ldquo;The network’s fixation on speculative harms is “almost like a caricature of the reality that we’re experiencing,” said Deborah Raji, an AI researcher at the University of California, Berkeley, who attended last month’s AI Insight Forum in the Senate. She worries that the focus on existential dangers will steer lawmakers away from addressing risks that today’s AI systems already pose, including their tendency to inject bias, spread misinformation, threaten copyright protections and weaken personal privacy.&rdquo;</em> - <a href="https://www.politico.com/news/2023/10/13/open-philanthropy-funding-ai-policy-00121362"><strong>How a billionaire-backed network of AI advisers took over Washington, POLITICO</strong></a></p>
</blockquote>
<h3 id="extensively-lobbied-regulations-benefit-incumbents">Extensively Lobbied Regulations Benefit Incumbents<a hidden class="anchor" aria-hidden="true" href="#extensively-lobbied-regulations-benefit-incumbents">#</a></h3>
<h4 id="the-effects-of-such-lobbying">The Effects of Such Lobbying<a hidden class="anchor" aria-hidden="true" href="#the-effects-of-such-lobbying">#</a></h4>
<p>With the influence of Big Tech AI incumbents and the ideas of &rsquo;existential threats&rsquo; evidently having a massive role to play in the drafting of AI regulation, one could expect policy proposals to benefit the interests of such parties.</p>
<p>So lets have a look at an example of a legislative proposal: namely one bipartisan bill proposed in the United States by <a href="https://www.blumenthal.senate.gov/newsroom/press/release/blumenthal-and-hawley-announce-bipartisan-framework-on-artificial-intelligence-legislation">U.S. Senators Richard Blumenthal (D-CT) and Josh Hawley (R-MO), Chair and Ranking Member of the Senate Judiciary Subcommittee on Privacy, Technology, and the Law.</a> Within this proposed legislative framework the key takeaways are:</p>
<ul>
<li>&ldquo;The establishment of a licencing regime administered by an independent oversight body&rdquo; which AI developers working on sophisticated general purpose models would be required to register with the body. In this context a licencing scheme is just another word for permission.</li>
<li>The above mentioned body would &ldquo;ensure legal accountability for harms&rdquo; through the requirement of AI companies being held liable when harms are caused through the use of their models.</li>
</ul>
<p>This legislation echoes the type of regulation called for by the likes of &lsquo;Open&rsquo;AI in the May 2023 Congressional hearing on AI.</p>
<blockquote>
<p><em>&ldquo;he (Sam Altman) supported the creation of a federal agency that can grant licenses to create AI models above a certain threshold of capabilities, and can also revoke those licenses if the models don’t meet safety guidelines set by the government.&rdquo;</em> - <a href="https://time.com/6280372/sam-altman-chatgpt-regulate-ai/"><strong>OpenAI CEO Sam Altman Asks Congress to Regulate AI, TIME</strong></a></p>
</blockquote>
<p>This establishment of a strict liability regime based upon existential harms caused by AI models would ensure that only large, already established incumbent companies would have the financial and technical ability to comply with the law, while new startups and alternative non corporate structures (such as open-source) would face serious barriers to entry. Such a legislative proposal, if passed into law, would almost certainly pose limits, if not entirely restrict the legal development of open-source AI while leaving only the big, closed and proprietary models standing.</p>
<h4 id="opposition-to-the-current-regulatory-narrative">Opposition to the Current Regulatory Narrative<a hidden class="anchor" aria-hidden="true" href="#opposition-to-the-current-regulatory-narrative">#</a></h4>
<p>This is a view shared by a growing number of key AI figures, academics and technologists. Perhaps the most notable of these is <a href="https://www.andrewng.org/">Andrew Ng:</a> Stanford Professor; machine learning teacher to &lsquo;Open&rsquo;AI CEO Sam Altman; co-founder of Google Brain and a globally recognized leader in Artificial Intelligence. He is of the belief that notions of Artificial Intelligence leading to the extinction of the human race is <a href="https://web.archive.org/web/20231030062420/https://www.afr.com/technology/google-brain-founder-says-big-tech-is-lying-about-ai-human-extinction-danger-20231027-p5efnz">&ldquo;a lie being promulgated by big tech in the hope of triggering heavy regulation that would shut down competition in the AI market&rdquo;</a> and a <a href="https://www.ft.com/content/2dc07f9e-d2a9-4d98-b746-b051f9352be3">big proponent of open-source AI development.</a></p>
<blockquote>
<p><em>&ldquo;Andrew Ng said that the “bad idea that AI could make us go extinct” was merging with the “bad idea that a good way to make AI safer is to impose burdensome licensing requirements” on the AI industry.&rdquo;</em> - <a href="https://web.archive.org/web/20231030062420/https://www.afr.com/technology/google-brain-founder-says-big-tech-is-lying-about-ai-human-extinction-danger-20231027-p5efnz"><strong>Google Brain founder says big tech is lying about AI extinction danger, Financial Review</strong> </a></p>
</blockquote>
<p>I would highly <a href="https://www.ft.com/content/2dc07f9e-d2a9-4d98-b746-b051f9352be3">advise a read of the interview with Andrew Ng from the Financial Times,</a> of which I will take key snippets. (Extracts copied below are for nonprofit educational purposes under <a href="https://copyright.gov/title17/92chap1.html#107">Section 107 of the Copyright Act: Fair Use</a> and <a href="https://www.ippt.eu/legal-texts/copyright-information-society-directive/article-5">Article 5: Information Society Directive</a>)</p>
<blockquote>
<p><em>&ldquo;Open-source software’s getting easy enough for most people to just install it and use it now. And it’s not that I’m obsessed about regulation — but if some of the regulators have their way, it’d be much harder to let open-source models like this keep up.&rdquo;</em></p>
</blockquote>
<blockquote>
<p><em>&ldquo;Some proposals, for instance, have reporting or even licensing requirements for LLMs. And while the big tech companies have the bandwidth to deal with complex compliance, smaller businesses just don’t.&rdquo;</em></p>
</blockquote>
<blockquote>
<p><em>&ldquo;When I think about the AI human extinction scenarios, when I speak with people who say they’re concerned about this, their concerns seem very vague. And no one seems to be able to articulate exactly how AI could kill us all.&rdquo;</em></p>
</blockquote>
<blockquote>
<p><em>&ldquo;There is also some chance that is absolutely non-zero of our radio signals causing aliens to find us and wipe us all out. But the chance is so small that we should not waste disproportionate resources to defend against that danger. And what I’m seeing is that we are spending vastly disproportionate resources against a risk that is almost zero.&rdquo;</em></p>
</blockquote>
<blockquote>
<p><em>&ldquo;Multiple companies are overhyping the threat narrative. For large businesses that would rather not compete with open-source, there is an incentive. For some non-profits, there is an incentive to hype up fears, hype up phantoms, and then raise funding to fight the phantoms they themselves conjured. And there are also some individuals who are definitely commanding more attention and larger speaker fees because of fear that they are helping to hype up. I think there are a few people who are sincere — mistaken but sincere — but on top of that there are significant financial incentives for one or multiple parties to hype up fear.&rdquo;</em></p>
</blockquote>
<blockquote>
<p><em>&ldquo;When lots of people signed [the Center for AI Safety statement] saying AI is dangerous like nuclear weapons, <a href="https://www.ft.com/content/084d5627-5193-4bdc-892e-ebf9e30b7ea3">the media covered that</a>. When there have been much more sensible statements — for example, <a href="https://open.mozilla.org/letter/">Mozilla</a> saying that open source is a great way to ensure AI safety — almost none of the media cover that.&rdquo;</em></p>
</blockquote>
<p>Andrew Ng is not alone in these views. In addition to the <a href="https://laion.ai/notes/letter-to-the-eu-parliament/">LAION</a> open letter addressed to the European Parliament calling for the protection of open-source AI that was referred to previously, Mozilla has also released a <a href="https://open.mozilla.org/letter/">Joint Statement on AI Safety and Openness</a> signed by a varied range of companies, civil society organisations and leading individuals in the realms of computer science, engineering, journalism and policy making.</p>
<blockquote>
<p><em>&ldquo;Yes, openly available models come with risks and vulnerabilities — AI models can be abused by malicious actors or deployed by ill-equipped developers. However, we have seen time and time again that the same holds true for proprietary technologies — and that increasing public access and scrutiny makes technology safer, not more dangerous. The idea that tight and proprietary control of foundational AI models is the only path to protecting us from society-scale harm is naive at best, dangerous at worst.&rdquo;</em> - <a href="https://open.mozilla.org/letter/"><strong>Joint Statement on AI Safety and Openness, Mozilla</strong></a></p>
</blockquote>
<blockquote>
<p><em>&ldquo;Further, history shows us that quickly rushing towards the wrong kind of regulation can lead to concentrations of power in ways that hurt competition and innovation. Open models can inform an open debate and improve policy making. If our objectives are safety, security and accountability, then openness and transparency are essential ingredients to get us there.</em>&rdquo; - <a href="https://open.mozilla.org/letter/"><strong>Joint Statement on AI Safety and Openness, Mozilla</strong></a></p>
</blockquote>
<p>In addition to this, there is growing research highlighting the concerns of Big Tech influenced regulation on open-source AI development and the resultant negative impacts upon competition such as this piece by the Carnegie Endowment:</p>
<blockquote>
<p><em>&ldquo;policy measures that address only speculative superintelligence concerns (and not more evolutionary AI policy challenges) are especially likely to impose steep costs in exchange for minimal benefits.&rdquo;</em> - <a href="https://carnegieendowment.org/posts/2023/09/how-hype-over-ai-superintelligence-could-lead-policy-astray?lang=en"><strong>How Hype Over AI Superintelligence Could Lead Policy Astray, Carnegie Endowment</strong> </a></p>
</blockquote>
<p>and the peer reviewed paper from Stanford in the George Washington Law Review: <em><a href="https://law.stanford.edu/publications/ai-regulation-has-its-own-alignment-problem-the-technical-and-institutional-feasibility-of-disclosure-registration-licensing-and-auditing/">&ldquo;AI Regulation Has Its Own Alignment Problem: The Technical and Institutional Feasibility of Disclosure, Registration, Licensing, and Auditing&rdquo;</a></em>:</p>
<blockquote>
<p><em>&ldquo;the potential of licensing to undermine competition, raise costs to consumers, enable industry capture, and gatekeep professions indicates AI licensing would create horizontal misalignment&hellip; AI licensing that places significant pre- and post-market burdens on companies may be prohibitively costly for smaller developers.&rdquo;</em></p>
</blockquote>
<blockquote>
<p><em>&ldquo;Licensing the development or deployment of AI thus has the potential to concentrate economic power in the hands of a few large companies&hellip; Licensing may heighten market concentration by advantaging more established incumbents who can more easily bear the licensing costs.&rdquo;</em></p>
</blockquote>
<blockquote>
<p><em>&ldquo;Concentration of market power could even exacerbate other harms arising from AI or undermine human values and regulatory objectives these policies aim to promote.&rdquo;</em></p>
</blockquote>
<blockquote>
<p><em>&ldquo;Licensing also creates tradeoffs between openness and control. Open access may provide for less control by enabling individuals with bad intentions or insufficient training to more easily access resources, but it may also increase the likelihood that critical issues with the technology are identified after release. Licensing may make it harder for users to expose harms, especially considering how openness provided mechanisms for discovering and addressing cybersecurity risks.&rdquo;</em></p>
</blockquote>
<blockquote>
<p><em>&ldquo;licensing regimes are particularly susceptible to capture. For example, research suggests that lobbying by physician interest groups is linked to a higher probability that a state will have occupational licensing in the healthcare industry. The potential for special interest groups to have outsized impact on AI licensing regimes is particularly worrisome given licensing may make the frontier of AI technology inaccessible to most.&rdquo;</em></p>
</blockquote>
<p>These views have not gone unnoticed. The European Union&rsquo;s landmark AI Act has <a href="https://www.europarl.europa.eu/legislative-train/theme-a-europe-fit-for-the-digital-age/file-regulation-on-artificial-intelligence">carve outs and exceptions for open-source AI development</a>. Only time will tell how far these exceptions go.</p>
<p>Given <a href="https://www.semianalysis.com/p/google-we-have-no-moat-and-neither">leaked documentation from a Google</a> engineer that warns it and &lsquo;Open&rsquo;AI could <a href="https://www.theguardian.com/technology/2023/may/05/google-engineer-open-source-technology-ai-openai-chatgpt">lose out to open-source</a> technology in the &lsquo;AI arms race&rsquo;, it is clear that Big Tech is concerned with the prospects of widely available open AI and is evidently lobbying hard for rules to be crafted to their benefit as an industry.</p>
<h4 id="law-enforcement-and-the-national-security-state-is-getting-their-way-as-well">Law Enforcement and the National Security State is Getting their Way as Well<a hidden class="anchor" aria-hidden="true" href="#law-enforcement-and-the-national-security-state-is-getting-their-way-as-well">#</a></h4>
<p>While not the main focus of this post, it is worthwhile to note that it is not just Big Tech and the AI industry that is trying to get/ getting their way when it comes to AI regulation and policy. Law enforcement and national security agencies, leaders and policy makers are also involved in setting the agenda. There is a clear <a href="https://www.opensecrets.org/revolving-door">revolving door</a> and incestuous relationship between the National Security State and the Big Tech Surveillance Capitalists, best exemplified by &lsquo;Open&rsquo;AI&rsquo;s recent <a href="https://www.theverge.com/2024/6/13/24178079/openai-board-paul-nakasone-nsa-safety">appointment</a> of former NSA Director and spook in chief Paul M. Nakasone to the position of the company&rsquo;s board and the &lsquo;Safety and Security Committee&rsquo;.</p>
<p>In the EU, civil society organizations have raised the alarm about vague terms, exceptions and carve outs that the AI Act affords to law enforcement and national security agencies allowing them to use AI and conduct biometric and facial recognition on a mass societal level. Some of these statements are linked below:</p>
<ul>
<li><a href="https://edri.org/our-work/eu-ai-act-fails-to-set-gold-standard-for-human-rights/">EU’s AI Act fails to set gold standard for human rights</a>, European Digital Rights</li>
<li><a href="https://edri.org/our-work/packed-with-loopholes-why-the-ai-act-fails-to-protect-civic-space-and-the-rule-of-law/">Packed with loopholes: Why the AI Act fails to protect civic space and the rule of law</a>, European Digital Rights and the European Center for Not-for-Profit Law</li>
<li><a href="https://edri.org/our-work/civil-society-statement-regulate-police-tech-ai-act/">EU lawmakers must regulate the harmful use of tech by law enforcement in the AI Act</a>, European Digital Rights</li>
<li><a href="https://www.accessnow.org/press-release/ai-act-failure-for-human-rights-victory-for-industry-and-law-enforcement/">The EU AI Act: a failure for human rights, a victory for industry and law enforcement</a>, Accessnow</li>
<li><a href="https://www.laquadrature.net/en/2024/05/22/with-the-ai-act-adopted-the-techno-solutionist-gold-rush-can-continue/">WITH THE AI ACT ADOPTED, THE TECHNO-SOLUTIONIST GOLD-RUSH CAN CONTINUE</a>, La Quadrature du Net</li>
</ul>
<h3 id="conclusions">Conclusions<a hidden class="anchor" aria-hidden="true" href="#conclusions">#</a></h3>
<p>There are clearly risks posed by the widespread adoption of Artificial Intelligence and in no way am I diminishing the work to address these. Risks posed by AI in the here and now are varied but very real. These include but are not limited to:</p>
<ul>
<li>Online political influence operations using AI generated content masquerading as genuine human created content to proliferate disinformation and push agendas.</li>
<li>Privacy concerns arising out of the usage of AI systems to sort through, link together and synthesize vast amounts of data to create detailed social scores and profiles of individuals.</li>
<li>Privacy concerns arising out of the usage of AI in the realm of surveillance, facial recognition and biometric identification and the risks of bias and abuse.</li>
<li>Concerns with Artificial Intelligence&rsquo;s application in armed conflict, regarding accountability and compliance with the legal obligations of war.</li>
<li>Concerns with AI in policing and national security matters, particularly &lsquo;predictive&rsquo; models.</li>
<li>The usage of AI agents to wage cyber wars, and attack critical infrastructure through cyber attacks.</li>
<li>The usage of AI in the commission of cyber crimes ranging from the generation of lifelike child sexual abuse material to AI enhanced phishing and hacks.</li>
</ul>
<p>I am no expert on matters relating to AI and I do not claim to be. What I will claim to have, however, is extensive knowledge on the current surveillance society we all reside in, where we are all subject to the <a href="https://www.theguardian.com/technology/2015/jul/23/panopticon-digital-surveillance-jeremy-bentham">invisible panopticon</a> brought about by the same mega corporations and State agencies (the Surveillance Capitalist - National Security State nexus) that is now developing AI and extensively influencing and guiding the regulation of such technologies. Forgive me if I have a large degree of skepticism towards these companies and institutions and their motivations. I think the record speaks for itself on how they behave and operate. If you think it does not I suggest taking a look at the readings under the <a href="http://localhost:1313/resources/" title="Privacy and Security Resources">Privacy and Security Resources</a> section of my website.</p>
<p>It is my view that ethical and regulatory considerations towards AI systems are not wholly unjustified and in many cases are certainly valid. However, they can, have been and will be utilized to impose limitations designed primarily to concentrate power over such systems in the hands of those who are in the position to benefit from their usage the most. The Surveillance Capitalist - National Security State institutional nexus does not want ordinary people to be able to deploy locally run and open-source AI&rsquo;s, as in such a scenario it would not be capable of extracting people&rsquo;s money, data and <a href="https://nymag.com/intelligencer/2019/02/shoshana-zuboff-q-and-a-the-age-of-surveillance-capital.html">behavioral surplus</a> nor retain their monopolistic grip on the digital domain.</p>
<p>The suggested risks posed by open AI systems and proposals to establish regulatory regimes and oversight bodies complete with licencing (permission) and strict liability schemes that will crush open AI development are purported to be for AI safety and the benefit of society. But this obscures the reality. The above mentioned nexus is not to be trusted, with them pretending to serve the common good under a guise of morality as they pursue their own self interests of profit and informational power, regardless of consequences for the public. This is what I believe to be readily transparent and clear.</p>
<p>This is also raises another question. If Artificial Intelligence systems are indeed an existential threat and far too dangerous for them to be open sourced, democratized and available to the public at large then on what grounds is it not likewise too dangerous for such power to be consolidated in the hand of Big Tech, the Surveillance Capitalists and the National Security State apparatus?</p>
<p>It is my opinion that the ultimate threat posed by AI is not some doomsday extinction event but rather the consolidation of Artificial Intelligence power in the hands of an exclusive high &lsquo;priesthood&rsquo;, without true accountability and transparency. We have seen what the internet looks like today; a locked down, commodified shell of its original aspirations that places us all as economic objects in the relentless drive to extract and monetize ever more behavioral surplus, that is Surveillance Capitalism, while <a href="http://localhost:1313/posts/wearebeingwatched/" title="We are being Watched">we are all being watched</a> under the omniscient eye of Big Brother(s). Artificial Intelligence has the potential to up end this relationship between us and informational power centers or to forever render us as informational commodities. So which will it be? Only time will tell but that future rests upon whether or not AI power is consolidated or dispersed within society.</p>
<hr>
<p><em>&ldquo;The liberty of a democracy is not safe if the people tolerate the growth of private power to a point where it becomes stronger than their democratic state itself. That, in its essence, is Fascism—ownership of Government by an individual, by a group, or by any other controlling private power.&rdquo;</em>  <strong>- <a href="https://www.presidency.ucsb.edu/documents/message-congress-curbing-monopolies">Franklin D. Roosevelt, 32nd President of the United States of America</a></strong></p>
<hr>
<p><em>Disclaimer: I do not, by any means, claim to be an expert in matters relating to privacy, security and law or offer what can be construed as guaranteed fool-proof advice. What I do offer is an insight into these matters from someone who is highly invested in personal privacy/ security themselves and who is studying technology law at the level of higher education.</em></p>
<hr>
<div style="text-align: center" id="custom-substack-embed"></div>


<script>
  window.CustomSubstackWidget = {
    substackUrl: "privseclaw.substack.com",
    placeholder: "example@yourmail.com",
    buttonText: "Subscribe Now!",
    theme: "custom",
    colors: {
      primary: "#0FA4AF",
      input: "#3E3E3E",
      email: "#15CAD8",
      text: "#3E3E3E",
    },

    

  };
</script>
<script src="https://substackapi.com/widget.js" async></script>
<hr>
<script type="text/javascript" src="https://latest.cactus.chat/cactus.js"></script>
<link rel="stylesheet" href="https://latest.cactus.chat/style.css" type="text/css">
<div id="comment-section"></div>
<script>
initComments({
  node: document.getElementById("comment-section"),
  defaultHomeserverUrl: "https://matrix.cactus.chat:8448",
  serverName: "cactus.chat",
  siteName: "<privseclaw.info>",
  commentSectionId: "bigtechandai"
})
</script>




  </div>

  <footer class="post-footer">
    <ul class="post-tags">
    </ul>
<nav class="paginav">
  <a class="prev" href="http://localhost:1313/posts/futokeyboard/">
    <span class="title">« Prev</span>
    <br>
    <span>FUTO Keyboard Review: An Android Keyboard that is Private and Functional</span>
  </a>
  <a class="next" href="http://localhost:1313/posts/encryptionunderattack/">
    <span class="title">Next »</span>
    <br>
    <span>Encryption Under Attack (Yet Again)</span>
  </a>
</nav>


<ul class="share-buttons">
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share The AI Panic: Big Tech, Open-source and Monopoly Power on x"
            href="https://x.com/intent/tweet/?text=The%20AI%20Panic%3a%20Big%20Tech%2c%20Open-source%20and%20Monopoly%20Power&amp;url=http%3a%2f%2flocalhost%3a1313%2fposts%2fbigtechandai%2f&amp;hashtags=">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M512 62.554 L 512 449.446 C 512 483.97 483.97 512 449.446 512 L 62.554 512 C 28.03 512 0 483.97 0 449.446 L 0 62.554 C 0 28.03 28.029 0 62.554 0 L 449.446 0 C 483.971 0 512 28.03 512 62.554 Z M 269.951 190.75 L 182.567 75.216 L 56 75.216 L 207.216 272.95 L 63.9 436.783 L 125.266 436.783 L 235.9 310.383 L 332.567 436.783 L 456 436.783 L 298.367 228.367 L 432.367 75.216 L 371.033 75.216 Z M 127.633 110 L 164.101 110 L 383.481 400.065 L 349.5 400.065 Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share The AI Panic: Big Tech, Open-source and Monopoly Power on linkedin"
            href="https://www.linkedin.com/shareArticle?mini=true&amp;url=http%3a%2f%2flocalhost%3a1313%2fposts%2fbigtechandai%2f&amp;title=The%20AI%20Panic%3a%20Big%20Tech%2c%20Open-source%20and%20Monopoly%20Power&amp;summary=The%20AI%20Panic%3a%20Big%20Tech%2c%20Open-source%20and%20Monopoly%20Power&amp;source=http%3a%2f%2flocalhost%3a1313%2fposts%2fbigtechandai%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-288.985,423.278l0,-225.717l-75.04,0l0,225.717l75.04,0Zm270.539,0l0,-129.439c0,-69.333 -37.018,-101.586 -86.381,-101.586c-39.804,0 -57.634,21.891 -67.617,37.266l0,-31.958l-75.021,0c0.995,21.181 0,225.717 0,225.717l75.02,0l0,-126.056c0,-6.748 0.486,-13.492 2.474,-18.315c5.414,-13.475 17.767,-27.434 38.494,-27.434c27.135,0 38.007,20.707 38.007,51.037l0,120.768l75.024,0Zm-307.552,-334.556c-25.674,0 -42.448,16.879 -42.448,39.002c0,21.658 16.264,39.002 41.455,39.002l0.484,0c26.165,0 42.452,-17.344 42.452,-39.002c-0.485,-22.092 -16.241,-38.954 -41.943,-39.002Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share The AI Panic: Big Tech, Open-source and Monopoly Power on reddit"
            href="https://reddit.com/submit?url=http%3a%2f%2flocalhost%3a1313%2fposts%2fbigtechandai%2f&title=The%20AI%20Panic%3a%20Big%20Tech%2c%20Open-source%20and%20Monopoly%20Power">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-3.446,265.638c0,-22.964 -18.616,-41.58 -41.58,-41.58c-11.211,0 -21.361,4.457 -28.841,11.666c-28.424,-20.508 -67.586,-33.757 -111.204,-35.278l18.941,-89.121l61.884,13.157c0.756,15.734 13.642,28.29 29.56,28.29c16.407,0 29.706,-13.299 29.706,-29.701c0,-16.403 -13.299,-29.702 -29.706,-29.702c-11.666,0 -21.657,6.792 -26.515,16.578l-69.105,-14.69c-1.922,-0.418 -3.939,-0.042 -5.585,1.036c-1.658,1.073 -2.811,2.761 -3.224,4.686l-21.152,99.438c-44.258,1.228 -84.046,14.494 -112.837,35.232c-7.468,-7.164 -17.589,-11.591 -28.757,-11.591c-22.965,0 -41.585,18.616 -41.585,41.58c0,16.896 10.095,31.41 24.568,37.918c-0.639,4.135 -0.99,8.328 -0.99,12.576c0,63.977 74.469,115.836 166.33,115.836c91.861,0 166.334,-51.859 166.334,-115.836c0,-4.218 -0.347,-8.387 -0.977,-12.493c14.564,-6.47 24.735,-21.034 24.735,-38.001Zm-119.474,108.193c-20.27,20.241 -59.115,21.816 -70.534,21.816c-11.428,0 -50.277,-1.575 -70.522,-21.82c-3.007,-3.008 -3.007,-7.882 0,-10.889c3.003,-2.999 7.882,-3.003 10.885,0c12.777,12.781 40.11,17.317 59.637,17.317c19.522,0 46.86,-4.536 59.657,-17.321c3.016,-2.999 7.886,-2.995 10.885,0.008c3.008,3.011 3.003,7.882 -0.008,10.889Zm-5.23,-48.781c-16.373,0 -29.701,-13.324 -29.701,-29.698c0,-16.381 13.328,-29.714 29.701,-29.714c16.378,0 29.706,13.333 29.706,29.714c0,16.374 -13.328,29.698 -29.706,29.698Zm-160.386,-29.702c0,-16.381 13.328,-29.71 29.714,-29.71c16.369,0 29.689,13.329 29.689,29.71c0,16.373 -13.32,29.693 -29.689,29.693c-16.386,0 -29.714,-13.32 -29.714,-29.693Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share The AI Panic: Big Tech, Open-source and Monopoly Power on facebook"
            href="https://facebook.com/sharer/sharer.php?u=http%3a%2f%2flocalhost%3a1313%2fposts%2fbigtechandai%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-106.468,0l0,-192.915l66.6,0l12.672,-82.621l-79.272,0l0,-53.617c0,-22.603 11.073,-44.636 46.58,-44.636l36.042,0l0,-70.34c0,0 -32.71,-5.582 -63.982,-5.582c-65.288,0 -107.96,39.569 -107.96,111.204l0,62.971l-72.573,0l0,82.621l72.573,0l0,192.915l-191.104,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share The AI Panic: Big Tech, Open-source and Monopoly Power on whatsapp"
            href="https://api.whatsapp.com/send?text=The%20AI%20Panic%3a%20Big%20Tech%2c%20Open-source%20and%20Monopoly%20Power%20-%20http%3a%2f%2flocalhost%3a1313%2fposts%2fbigtechandai%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-58.673,127.703c-33.842,-33.881 -78.847,-52.548 -126.798,-52.568c-98.799,0 -179.21,80.405 -179.249,179.234c-0.013,31.593 8.241,62.428 23.927,89.612l-25.429,92.884l95.021,-24.925c26.181,14.28 55.659,21.807 85.658,21.816l0.074,0c98.789,0 179.206,-80.413 179.247,-179.243c0.018,-47.895 -18.61,-92.93 -52.451,-126.81Zm-126.797,275.782l-0.06,0c-26.734,-0.01 -52.954,-7.193 -75.828,-20.767l-5.441,-3.229l-56.386,14.792l15.05,-54.977l-3.542,-5.637c-14.913,-23.72 -22.791,-51.136 -22.779,-79.287c0.033,-82.142 66.867,-148.971 149.046,-148.971c39.793,0.014 77.199,15.531 105.329,43.692c28.128,28.16 43.609,65.592 43.594,105.4c-0.034,82.149 -66.866,148.983 -148.983,148.984Zm81.721,-111.581c-4.479,-2.242 -26.499,-13.075 -30.604,-14.571c-4.105,-1.495 -7.091,-2.241 -10.077,2.241c-2.986,4.483 -11.569,14.572 -14.182,17.562c-2.612,2.988 -5.225,3.364 -9.703,1.12c-4.479,-2.241 -18.91,-6.97 -36.017,-22.23c-13.314,-11.876 -22.304,-26.542 -24.916,-31.026c-2.612,-4.484 -0.279,-6.908 1.963,-9.14c2.016,-2.007 4.48,-5.232 6.719,-7.847c2.24,-2.615 2.986,-4.484 4.479,-7.472c1.493,-2.99 0.747,-5.604 -0.374,-7.846c-1.119,-2.241 -10.077,-24.288 -13.809,-33.256c-3.635,-8.733 -7.327,-7.55 -10.077,-7.688c-2.609,-0.13 -5.598,-0.158 -8.583,-0.158c-2.986,0 -7.839,1.121 -11.944,5.604c-4.105,4.484 -15.675,15.32 -15.675,37.364c0,22.046 16.048,43.342 18.287,46.332c2.24,2.99 31.582,48.227 76.511,67.627c10.685,4.615 19.028,7.371 25.533,9.434c10.728,3.41 20.492,2.929 28.209,1.775c8.605,-1.285 26.499,-10.833 30.231,-21.295c3.732,-10.464 3.732,-19.431 2.612,-21.298c-1.119,-1.869 -4.105,-2.99 -8.583,-5.232Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share The AI Panic: Big Tech, Open-source and Monopoly Power on telegram"
            href="https://telegram.me/share/url?text=The%20AI%20Panic%3a%20Big%20Tech%2c%20Open-source%20and%20Monopoly%20Power&amp;url=http%3a%2f%2flocalhost%3a1313%2fposts%2fbigtechandai%2f">
            <svg version="1.1" xml:space="preserve" viewBox="2 2 28 28" height="30px" width="30px" fill="currentColor">
                <path
                    d="M26.49,29.86H5.5a3.37,3.37,0,0,1-2.47-1,3.35,3.35,0,0,1-1-2.47V5.48A3.36,3.36,0,0,1,3,3,3.37,3.37,0,0,1,5.5,2h21A3.38,3.38,0,0,1,29,3a3.36,3.36,0,0,1,1,2.46V26.37a3.35,3.35,0,0,1-1,2.47A3.38,3.38,0,0,1,26.49,29.86Zm-5.38-6.71a.79.79,0,0,0,.85-.66L24.73,9.24a.55.55,0,0,0-.18-.46.62.62,0,0,0-.41-.17q-.08,0-16.53,6.11a.59.59,0,0,0-.41.59.57.57,0,0,0,.43.52l4,1.24,1.61,4.83a.62.62,0,0,0,.63.43.56.56,0,0,0,.4-.17L16.54,20l4.09,3A.9.9,0,0,0,21.11,23.15ZM13.8,20.71l-1.21-4q8.72-5.55,8.78-5.55c.15,0,.23,0,.23.16a.18.18,0,0,1,0,.06s-2.51,2.3-7.52,6.8Z" />
            </svg>
        </a>
    </li>
</ul>

  </footer>
</article>
    </main>
    
<footer class="footer">
        <span>&copy; 2025 <a href="http://localhost:1313/">Privacy, Security and the Law</a></span> · 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
